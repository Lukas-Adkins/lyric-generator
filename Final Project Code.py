{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Final Project Code","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"Zp85dxDgfoZ0"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive', force_remount=True)\n","\n","import csv\n","from keras.layers import Dense, Embedding, LSTM\n","from keras.models import Sequential, load_model\n","from keras.preprocessing.text import Tokenizer\n","from keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras import utils as np_utils\n","import numpy as np\n","import pickle\n","import spacy \n","from spacy.lang.en import English\n","import string\n","import sys\n","from pandas import Series\n","from sklearn.preprocessing import StandardScaler\n","from math import sqrt\n","\n","\n","nlp = spacy.load(\"en_core_web_sm\") \n","\n","# Returns songs where start <= data < end\n","def get_songs(start, end):\n","  # csv has a max field limit that is by default, greater than our data size\n","  # So this was added to increase that max \n","  maxInt = sys.maxsize\n","  while True:\n","    try: \n","      csv.field_size_limit(maxInt)\n","      break\n","    except OverflowError:\n","      maxInt = int(maxInt/10)\n","\n","  lyrics = open('/content/gdrive/MyDrive/Colab Notebooks/CS404 Final Project/lyrics.csv', mode='r')\n","  csv_reader = csv.reader(lyrics)\n","  # skip header\n","  next(csv_reader)\n","\n","  songs = []\n","  count = 0\n","  for line in csv_reader:\n","    if count == 0:\n","      # append top song of the week\n","      date = line[0].split(\"-\")\n","      temp = date\n","      year = int(date[0])\n","      if year >= start and year < end:\n","        count = count + 1\n","        songs.append(line)\n","    else:\n","      # skip the rest\n","      temp = line[0].split(\"-\")\n","      if temp != date:\n","        count = 0\n","      else:\n","        count = 1\n","  return songs\n","\n","# Get lyrics tokens\n","def get_tokens(lyrics):\n","  tokenList = []\n","  for line in lyrics:\n","    words = []\n","    temp = line.split()\n","    for i in range(len(temp)):\n","      words.append(temp[i])\n","    tokenList.append(words)\n","  return tokenList\n","\n","def get_sequences(lyrics):\n","  tokens = lyrics.split()\n","  sequences = []\n","  for i in range(1, 51):\n","    seq = tokens[:i+1]\n","    sequences.append(seq)\n","  for i in range(51, len(tokens)):\n","    seq = tokens[i-50:i+1]\n","    sequences.append(seq)\n","  tokenizer.fit_on_texts(sequences)\n","  tokenSequences = tokenizer.texts_to_sequences(sequences)\n","  tokenCount = len(tokenizer.word_index) + 1\n","  return tokenSequences, tokenCount\n","\n","# Define LSTM model\n","def create_model(tokenCount):\n","  model = Sequential() # Model type: Stack of layers, each has exactly 1 input and 1 output tensor\n","  model.add(Embedding(tokenCount, 50, input_length=50))\n","  model.add(LSTM(100)) \n","  model.add(Dense(tokenCount))\n","  model.compile(loss='categorical_crossentropy') \n","  model.summary()\n","  return model\n","\n","def lyrics_to_sequences(songLyric):\n","  tokens = songLyric.split()\n","  sequences = []\n","  for i in range(1, 51):\n","    seq = tokens[:i+1]\n","    sequences.append(seq)\n","  for i in range(51, len(tokens)):\n","    seq = tokens[i-50:i+1]\n","    sequences.append(seq)\n","  return sequences\n","  \n","def softmax(x): #computes softmax for 1d array\n","    ex = np.exp(x - np.max(x))\n","    return ex / ex.sum()\n","\n","# Generate Sentences\n","def generate_sentence(model, numWords, tokenizer, seed):\n","  result = []\n","  input = seed\n","  for i in range(numWords):\n","    #print(result)\n","    token = tokenizer.texts_to_sequences([input])\n","    token = pad_sequences(token, maxlen=50, padding='pre')\n","    prediction = model.predict(token)[0]\n","    prediction = (prediction - np.min(prediction)) / (np.max(prediction) - np.min(prediction))\n","\n","    prediction = softmax(prediction);\n","    prediction = np.random.choice(len(prediction), p=prediction)\n","\n","    output = ''\n","    for word, index in tokenizer.word_index.items():\n","      if index == prediction:\n","        output = word\n","        break\n","    input += \" \" + output\n","    result.append(output)\n","  return ' '.join(result)\n","\n","def clean_up(lyrics, lang):\n","  lyrics = lyrics.lower()\n","  tokenizer = nlp.Defaults.create_tokenizer(lang)\n","  tokens = tokenizer(lyrics)\n","  result = []\n","  for token in tokens:\n","    if not token.is_punct: \n","      result.append(token.orth_)\n","  return ' '.join(result)\n","\n","def train_model(model, trainingData, trainLabels, tokenCount):\n","  # Get labels\n","  trainLength = 1000\n","  count = 0\n","  if trainLength > tokenCount:\n","    trainLabels = np_utils.to_categorical(labels, num_classes=tokenCount)\n","    model.train_on_batch(trainingData, trainLabels)\n","  else:\n","    # train labels in increments of trainLength\n","    for i in range(0, tokenCount):\n","      trainLabels = np_utils.to_categorical(labels[i:trainLength], num_classes=tokenCount)\n","      model.train_on_batch(trainingData[i:trainLength], trainLabels)\n","      count = i\n","      i = i + trainLength\n","    # train the leftover\n","    leftover = tokenCount-count\n","    print(leftover)\n","    trainLabels = np_utils.to_categorical(labels[count:tokenCount], num_classes=tokenCount)\n","    model.train_on_batch(trainingData[count:tokenCount], trainLabels)\n","    model.save(\"/content/gdrive/MyDrive/Colab Notebooks/CS404 Final Project/test.h5\")\n","  \n","if __name__ == \"__main__\":\n","  start = 2010\n","  end = 2020\n","  songs = get_songs(start, end)\n","  # Remove duplicates\n","  uniqueSongs = []\n","  for song in songs:\n","    if song not in uniqueSongs:\n","      uniqueSongs.append(song)\n","  lyrics = []\n","  lang = English()\n","  for song in uniqueSongs:\n","    lyrics.append(clean_up(song[3], lang))\n","\n","  trainingData = []\n","  for song in lyrics:\n","    sequences = lyrics_to_sequences(song)\n","    for seq in sequences:\n","      trainingData.append(seq)\n","\n","  tokenizer = Tokenizer()\n","  tokenizer.fit_on_texts(trainingData)\n","  tokenCount = len(tokenizer.word_index) + 1\n","  trainingData = tokenizer.texts_to_sequences(trainingData)\n","  trainingData = pad_sequences(trainingData, padding='pre')\n","  trainingData, labels = trainingData[:,:-1],trainingData[:,-1]\n","  \n","  # save tokenizer\n","  with open('/content/gdrive/MyDrive/Colab Notebooks/CS404 Final Project/tokenizer10-20.pickle', 'wb') as handle:\n","    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\n","\n","  model = create_model(tokenCount);\n","\n","  #for i in range(len(trainingData)):\n","    #data = np.array(trainingData[i])\n","  #trainingData = (trainingData - np.min(trainingData)) / (np.max(trainingData) - np.min(trainingData))\n","    #print(normalized_dataset)\n","    #trainingData[i] = normalized_dataset\n","  \n","  # Function that trains the model using train_on_batch\n","  # train_model(model, trainingData, labels, tokenCount)\n","\n","  labels = np_utils.to_categorical(labels, num_classes=tokenCount)\n","  model.fit(trainingData, labels, epochs=5)\n","  \n","  # save model\n","  model.save(\"/content/gdrive/MyDrive/Colab Notebooks/CS404 Final Project/model10-20.h5\")\n","\n","  for i in range(0, 10):\n","    genSeq = generate_sentence(model, 100, tokenizer, \"together\");\n","    print(genSeq)\n","    print()"],"execution_count":null,"outputs":[]}]}